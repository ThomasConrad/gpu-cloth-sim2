# Task 017: Performance Profiling Integration

**Epic:** CLOTH-001  
**Milestone:** 5 - Optimization  
**Effort:** 2 days (5 story points)  
**Priority:** Medium  
**Parallelizable:** Yes - Independent profiling system  

## Overview

Implement comprehensive performance profiling integration with GPU timing using timestamp queries, memory bandwidth utilization measurement, bottleneck identification and reporting, and performance regression testing framework. This system will provide detailed insights for optimization work.

## Technical Requirements

### GPU Timing Integration
- GPU timestamp query implementation for accurate timing
- Per-shader timing with statistical analysis
- Frame timing breakdown with detailed pipeline stages
- Cross-platform timing support (D3D12, Vulkan, Metal)

### Memory Bandwidth Measurement
- Real-time memory bandwidth utilization tracking
- Buffer access pattern analysis
- Cache hit/miss ratio monitoring
- Memory pressure detection and reporting

### Performance Monitoring Framework
```rust
struct PerformanceProfiler {
    gpu_timer: GPUTimestampManager,
    memory_profiler: MemoryBandwidthProfiler,
    bottleneck_analyzer: BottleneckAnalyzer,
    regression_tester: RegressionTestFramework,
    metrics_collector: MetricsCollector,
}

struct ProfiledFrame {
    frame_number: u64,
    total_frame_time: Duration,
    physics_time: Duration,
    rendering_time: Duration,
    memory_bandwidth: f32,
    gpu_utilization: f32,
    bottlenecks: Vec<PerformanceBottleneck>,
}
```

## Implementation Details

### GPU Timestamp Management
```rust
struct GPUTimestampManager {
    timestamp_pool: Vec<wgpu::QuerySet>,
    timestamp_buffers: Vec<wgpu::Buffer>,
    query_capacity: u32,
    current_query_index: usize,
    pending_queries: VecDeque<PendingQuery>,
}

struct PendingQuery {
    name: String,
    start_query_id: u32,
    end_query_id: u32,
    frame_id: u64,
}

impl GPUTimestampManager {
    fn begin_timing(&mut self, encoder: &mut wgpu::CommandEncoder, name: &str) -> TimingHandle {
        let query_id = self.allocate_query_pair();
        
        encoder.write_timestamp(&self.timestamp_pool[0], query_id);
        
        TimingHandle {
            name: name.to_string(),
            start_query_id: query_id,
            manager: self,
        }
    }
    
    fn end_timing(&mut self, encoder: &mut wgpu::CommandEncoder, handle: TimingHandle) {
        encoder.write_timestamp(&self.timestamp_pool[0], handle.start_query_id + 1);
        
        self.pending_queries.push_back(PendingQuery {
            name: handle.name,
            start_query_id: handle.start_query_id,
            end_query_id: handle.start_query_id + 1,
            frame_id: self.current_frame_id,
        });
    }
    
    fn collect_results(&mut self) -> Vec<TimingResult> {
        let mut results = Vec::new();
        
        while let Some(query) = self.pending_queries.front() {
            if let Some(timing) = self.try_read_timestamp(query) {
                results.push(TimingResult {
                    name: query.name.clone(),
                    duration: timing,
                    frame_id: query.frame_id,
                });
                self.pending_queries.pop_front();
            } else {
                break; // Results not ready yet
            }
        }
        
        results
    }
}
```

### Memory Bandwidth Profiling
```rust
struct MemoryBandwidthProfiler {
    buffer_access_tracker: HashMap<String, BufferAccessInfo>,
    bandwidth_history: RingBuffer<f32, 240>, // 4 seconds at 60 FPS
    current_frame_transfers: u64,
    theoretical_peak_bandwidth: f32,
}

struct BufferAccessInfo {
    read_bytes: u64,
    write_bytes: u64,
    access_pattern: AccessPattern,
    cache_efficiency: f32,
}

impl MemoryBandwidthProfiler {
    fn track_buffer_access(&mut self, buffer_name: &str, operation: MemoryOperation) {
        let access_info = self.buffer_access_tracker
            .entry(buffer_name.to_string())
            .or_insert(BufferAccessInfo::default());
        
        match operation {
            MemoryOperation::Read { bytes, pattern } => {
                access_info.read_bytes += bytes;
                access_info.update_access_pattern(pattern);
            }
            MemoryOperation::Write { bytes, pattern } => {
                access_info.write_bytes += bytes;
                access_info.update_access_pattern(pattern);
            }
        }
        
        self.current_frame_transfers += operation.bytes();
    }
    
    fn calculate_bandwidth_utilization(&mut self, frame_time: Duration) -> f32 {
        let bytes_per_second = self.current_frame_transfers as f32 / frame_time.as_secs_f32();
        let utilization = bytes_per_second / self.theoretical_peak_bandwidth;
        
        self.bandwidth_history.push(utilization);
        self.current_frame_transfers = 0;
        
        utilization
    }
    
    fn analyze_memory_bottlenecks(&self) -> Vec<MemoryBottleneck> {
        let mut bottlenecks = Vec::new();
        
        for (buffer_name, access_info) in &self.buffer_access_tracker {
            if access_info.cache_efficiency < 0.6 {
                bottlenecks.push(MemoryBottleneck {
                    buffer_name: buffer_name.clone(),
                    issue: BottleneckIssue::PoorCacheEfficiency,
                    severity: (1.0 - access_info.cache_efficiency) * 100.0,
                    suggestion: "Consider data layout optimization or access pattern changes".to_string(),
                });
            }
            
            if access_info.read_bytes > access_info.write_bytes * 10 {
                bottlenecks.push(MemoryBottleneck {
                    buffer_name: buffer_name.clone(),
                    issue: BottleneckIssue::ReadHeavyAccess,
                    severity: (access_info.read_bytes as f32 / access_info.write_bytes as f32).log10() * 20.0,
                    suggestion: "Consider caching frequently read data or reducing read frequency".to_string(),
                });
            }
        }
        
        bottlenecks
    }
}
```

### Bottleneck Analysis System
```rust
struct BottleneckAnalyzer {
    frame_timing_history: RingBuffer<FrameTiming, 300>,
    bottleneck_detector: BottleneckDetector,
    performance_thresholds: PerformanceThresholds,
}

struct FrameTiming {
    physics_ms: f32,
    rendering_ms: f32,
    memory_bandwidth_usage: f32,
    gpu_utilization: f32,
    cpu_wait_time: f32,
}

impl BottleneckAnalyzer {
    fn analyze_performance_bottlenecks(&mut self, current_frame: &FrameTiming) -> BottleneckReport {
        self.frame_timing_history.push(*current_frame);
        
        let mut bottlenecks = Vec::new();
        let avg_timing = self.calculate_average_timing();
        
        // Detect GPU bottlenecks
        if avg_timing.physics_ms > self.performance_thresholds.physics_ms {
            bottlenecks.push(PerformanceBottleneck {
                component: Component::PhysicsCompute,
                severity: (avg_timing.physics_ms / self.performance_thresholds.physics_ms - 1.0) * 100.0,
                description: "Physics computation is taking longer than target".to_string(),
                suggestions: vec![
                    "Reduce solver iterations".to_string(),
                    "Optimize constraint solving parallelization".to_string(),
                    "Consider LOD for distant cloth".to_string(),
                ],
            });
        }
        
        // Detect memory bottlenecks
        if avg_timing.memory_bandwidth_usage > 0.9 {
            bottlenecks.push(PerformanceBottleneck {
                component: Component::MemoryBandwidth,
                severity: (avg_timing.memory_bandwidth_usage - 0.9) * 1000.0,
                description: "Memory bandwidth utilization is very high".to_string(),
                suggestions: vec![
                    "Optimize memory access patterns".to_string(),
                    "Reduce buffer sizes where possible".to_string(),
                    "Implement data compression".to_string(),
                ],
            });
        }
        
        // Detect CPU bottlenecks
        if avg_timing.cpu_wait_time > 2.0 {
            bottlenecks.push(PerformanceBottleneck {
                component: Component::CPUGPUSync,
                severity: avg_timing.cpu_wait_time * 10.0,
                description: "CPU is waiting too long for GPU".to_string(),
                suggestions: vec![
                    "Reduce CPU-GPU synchronization points".to_string(),
                    "Implement double buffering".to_string(),
                    "Optimize GPU workload distribution".to_string(),
                ],
            });
        }
        
        BottleneckReport {
            frame_id: self.current_frame_id,
            primary_bottleneck: self.identify_primary_bottleneck(&bottlenecks),
            all_bottlenecks: bottlenecks,
            overall_score: self.calculate_performance_score(&avg_timing),
        }
    }
}
```

### Performance Regression Testing
```rust
struct RegressionTestFramework {
    baseline_metrics: HashMap<String, BaselineMetric>,
    test_scenarios: Vec<TestScenario>,
    regression_threshold: f32, // 5% performance regression threshold
}

struct BaselineMetric {
    name: String,
    baseline_value: f32,
    unit: String,
    last_updated: chrono::DateTime<chrono::Utc>,
}

impl RegressionTestFramework {
    fn run_regression_tests(&mut self) -> RegressionReport {
        let mut test_results = Vec::new();
        
        for scenario in &self.test_scenarios {
            let current_result = self.run_test_scenario(scenario);
            
            if let Some(baseline) = self.baseline_metrics.get(&scenario.name) {
                let performance_change = (current_result.value - baseline.baseline_value) 
                    / baseline.baseline_value * 100.0;
                
                let status = if performance_change > self.regression_threshold {
                    TestStatus::Regression
                } else if performance_change < -self.regression_threshold {
                    TestStatus::Improvement
                } else {
                    TestStatus::Stable
                };
                
                test_results.push(RegressionTestResult {
                    scenario_name: scenario.name.clone(),
                    current_value: current_result.value,
                    baseline_value: baseline.baseline_value,
                    performance_change,
                    status,
                    details: current_result.details,
                });
            }
        }
        
        RegressionReport {
            test_date: chrono::Utc::now(),
            total_tests: test_results.len(),
            regressions: test_results.iter().filter(|r| matches!(r.status, TestStatus::Regression)).count(),
            improvements: test_results.iter().filter(|r| matches!(r.status, TestStatus::Improvement)).count(),
            results: test_results,
        }
    }
    
    fn update_baselines(&mut self, approved_results: &[RegressionTestResult]) {
        for result in approved_results {
            if let Some(baseline) = self.baseline_metrics.get_mut(&result.scenario_name) {
                baseline.baseline_value = result.current_value;
                baseline.last_updated = chrono::Utc::now();
            }
        }
        
        self.save_baselines_to_disk();
    }
}
```

## Acceptance Criteria

- [ ] **GPU Timing**: Accurate GPU timestamp queries provide per-shader timing data
- [ ] **Memory Analysis**: Memory bandwidth utilization measurement with bottleneck detection
- [ ] **Bottleneck Detection**: Automatic identification of performance bottlenecks with actionable suggestions
- [ ] **Regression Testing**: Automated performance regression detection with configurable thresholds
- [ ] **Cross-Platform**: Profiling works consistently across Windows, macOS, and Linux
- [ ] **Minimal Overhead**: Profiling system adds <2% performance overhead when enabled

## Dependencies

### Prerequisites
- Task 015 (GPU Compute Optimization) - uses profiling for optimization validation
- Task 016 (Scalability Enhancements) - uses profiling for scalability analysis
- Task 001 (GPU Buffer Management System) - memory profiling integration

### Enhances
- All previous tasks - provides performance analysis capability
- Future optimization work - provides measurement framework

### Can Work In Parallel With
- Task 018 (Comprehensive Documentation) - different deliverable area
- Task 019 (Unit Tests for Mathematical Operations) - different testing area

## Testing Strategy

### Profiling Accuracy Tests
```rust
#[test]
fn gpu_timing_accuracy() {
    // Test that GPU timing measurements are consistent and accurate
}

#[test]
fn memory_bandwidth_calculation() {
    // Test memory bandwidth calculation accuracy
}

#[test]
fn bottleneck_detection_accuracy() {
    // Test that bottleneck detection correctly identifies known issues
}

#[test]
fn regression_test_consistency() {
    // Test that regression tests produce consistent results
}
```

### Performance Validation
- Verify profiling overhead is minimal
- Test profiling accuracy across different hardware
- Validate bottleneck suggestions lead to actual improvements
- Test regression detection sensitivity and specificity

## Technical Specifications

### Profiling Data Format
```rust
struct ProfilingSession {
    session_id: String,
    start_time: chrono::DateTime<chrono::Utc>,
    hardware_info: HardwareInfo,
    software_version: String,
    frame_data: Vec<ProfiledFrame>,
    summary_statistics: SummaryStatistics,
}

struct SummaryStatistics {
    average_frame_time: f32,
    percentile_99_frame_time: f32,
    average_memory_bandwidth: f32,
    primary_bottlenecks: Vec<PerformanceBottleneck>,
}
```

### Cross-Platform Timing
```rust
trait PlatformTimer {
    fn create_timestamp_query(&self) -> Box<dyn TimestampQuery>;
    fn read_timestamp(&self, query: &dyn TimestampQuery) -> Option<Duration>;
    fn get_timestamp_frequency(&self) -> u64;
}

#[cfg(target_os = "windows")]
struct D3D12Timer;

#[cfg(target_os = "macos")]
struct MetalTimer;

#[cfg(target_os = "linux")]
struct VulkanTimer;
```

## Performance Targets

- **Profiling Overhead**: <2% performance impact when profiling enabled
- **Timing Precision**: Microsecond precision for GPU timing measurements
- **Memory Analysis**: Real-time bandwidth utilization with <1% error
- **Bottleneck Detection**: Identify primary bottleneck within 5 seconds of occurrence
- **Regression Detection**: 95% accuracy in detecting significant performance changes

## Risk Factors

- **Platform Differences**: GPU timing behavior may vary significantly across platforms
- **Profiling Overhead**: Detailed profiling may impact the performance being measured
- **Hardware Limitations**: Some hardware may not support all profiling features
- **Data Volume**: Extensive profiling data may consume significant storage

## Implementation Notes

### GPU Timing Strategy
- Use GPU timestamp queries where available, fall back to CPU timing
- Implement query pool management to handle multiple concurrent measurements
- Consider temporal smoothing to reduce measurement noise
- Handle GPU preemption and context switching gracefully

### Memory Profiling Approach
- Focus on coarse-grained memory bandwidth rather than detailed memory access traces
- Use hardware performance counters where available
- Estimate bandwidth usage from buffer sizes and access patterns
- Consider memory controller utilization vs memory bandwidth distinction

### Bottleneck Analysis Design
- Implement hierarchical bottleneck analysis (system → component → specific issue)
- Provide confidence levels for bottleneck identification
- Consider temporal patterns in bottleneck detection
- Focus on actionable bottlenecks rather than theoretical limits

### Regression Testing Strategy
- Maintain separate baselines for different hardware configurations
- Use statistical significance testing for regression detection
- Implement automated baseline updates for approved performance changes
- Consider environmental factors (thermal throttling, system load) in testing